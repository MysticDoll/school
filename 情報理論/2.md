# 様々な情報量
## エントロピー
得た情報の量 = 減った不確かさの量

### 定義
アルファベットx上に値をとる離散確率変数Xに対して

```math
H(X) = - \sum_{x \in \chi} p_{X} (\chi) \log p_{X} (x)
```

ただし

```math
p_{X} (x) = P(X = x)
```

logの底は2

#### 例
```math
P(X = 0) = 1/4 \\
P(X = 1) = 1/2 \\
P(X = 2) = 1/4 \\
H(X) = - \frac{1}{4} \log \frac{1}{4} - \frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} \\
     = \frac{3}{2}
```


### エントロピーの性質
- 非負性
```math
H(X) \geq 0 \\

∵ \ - p \log p \geq 0 (0 \leq p \leq 1) \\
```

さらに

```math
H(X) = 0 \Leftrightarrow - p_X (x) \log p_X (x) = 0 \\
         \Leftrightarrow すべての x \in \chi に対して p_X (x) = 0 または p_X (x) = 1 \\
         \Leftrightarrow ある x \in \chi に対して p_X (x) = 1
```

#### 例
X が x 上の一様分布に従うとき

```math
H(X) = - \sum_{x \in \chi} \frac{1}{|x|} \log \frac{1}{|x} \\
     = |x| \times \frac{1}{x} \log |x| \\
     = log |x|
```
#### 例
```math
|x| = 2 のとき、例えば \\
x = {0, 1} のとき \\
p_X(x) = \begin{cases}
  t & x = 0 \\
  1 - t & x = 1
\end{cases}

(0 \leq t \leq 1) \\

のとき \\

H(X) = - t \log t  - (1 - t) \log (1 - t) = h(t) とおく(2値エントロピー関数)
```

##### 注
```math
H(X) = \sum_{x \in \chi} p_{X} (x) ( - \log p_{X} (x) )　\\
     = E [ - \log p_{X} (X) ]
```
