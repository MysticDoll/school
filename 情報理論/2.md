# 様々な情報量
## エントロピー
得た情報の量 = 減った不確かさの量

### 定義
アルファベットx上に値をとる離散確率変数Xに対して

$$
H(X) = - \sum\_{x \in \chi} p\_{X} (\chi) \log p\_{X} (x)
$$

ただし

$$
p\_{X} (x) = P(X = x)
$$

logの底は2

#### 例
$$
P(X = 0) = 1/4 \\
P(X = 1) = 1/2 \\
P(X = 2) = 1/4 \\
H(X) = - \frac{1}{4} \log \frac{1}{4} - \frac{1}{2} \log \frac{1}{2} - \frac{1}{4} \log \frac{1}{4} \\
     = \frac{3}{2}
$$


### エントロピーの性質
- 非負性

$$
H(X) \geq 0 \\
∵ \ - p \log p \geq 0 (0 \leq p \leq 1) \\
$$

さらに

$$
H(X) = 0 \Leftrightarrow - p\_X (x) \log p\_X (x) = 0 \\
         \Leftrightarrow すべての x \in \chi に対して p\_X (x) = 0 または p\_X (x) = 1 \\
         \Leftrightarrow ある x \in \chi に対して p\_X (x) = 1
$$

#### 例
X が x 上の一様分布に従うとき

$$
H(X) = - \sum\_{x \in \chi} \frac{1}{|x|} \log \frac{1}{|x} \\
     = |x| \times \frac{1}{x} \log |x| \\
     = log |x|
$$

#### 例

$$
|x| = 2 のとき、例えば \\
x = {0, 1} のとき \\
p\_X(x) = \begin{cases}
  t & x = 0 \\
  1 - t & x = 1
\end{cases}
(0 \leq t \leq 1) \\
のとき \\
H(X) = - t \log t  - (1 - t) \log (1 - t) = h(t) とおく(2値エントロピー関数)
$$

##### 注
$$
H(X) = \sum\_{x \in \chi} p\_{X} (x) ( - \log p\_{X} (x) )　\
     = E [ - \log p\_{X} (X) ]
$$
